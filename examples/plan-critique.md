# Example Devil's Advocate Analysis: Database Migration Plan

> **Original plan:** Migrate production PostgreSQL 13 database (2TB, 40M daily active records) to PostgreSQL 16 on a new cloud provider during a 4-hour maintenance window on a Sunday.

---

## ‚ö° IMMEDIATE REPORT ‚Äî üî¥ Critical Finding

**Finding**: Rollback is not physically feasible ‚Äî restoring a 2TB database takes 2‚Äì4 hours, which cannot fit inside the remaining maintenance window after any migration attempt.
**Domain**: Architecture
**Why immediate**: Irreversible ‚Äî if the migration fails at hour 3, there is no recovery path. The team is forced to either go live on a potentially corrupted target or extend the outage into Monday business hours.
**Risk if not addressed**: Uncontrolled production outage beyond the maintenance window, with no safe state to return to.

---
### üîç Context Needed

1. **Dry-run performed?** Has the full migration been tested on a production-sized clone? What was the actual elapsed time?
2. **Rollback plan**: If pg_restore fails at hour 3, what is the explicit decision tree? Continue, extend window, or keep old primary live?
3. **Logical replication feasibility**: Is the target cloud provider's PostgreSQL 16 instance compatible with pglogical or built-in logical replication from the source?
4. **Downstream consumers**: How many ETL jobs, reporting pipelines, and application connection strings point to the current database endpoint?

---
Reply: üìù [answers] to raise confidence | `continue` to proceed at worst-case score

*(For this example, context was provided: "No dry-run performed on production-sized clone. No explicit rollback decision tree exists. Target provider supports logical replication. 14 downstream consumers identified.")*

---

## üõë HANDBRAKE ACTIVATED

**Critical finding**: No viable rollback strategy for a 2TB database within a 4-hour window
**Domain**: Architecture / Database
**Responsible role**: DBA / Platform Architect
**Why Handbrake level**: Irreversible ‚Äî once a 2TB pg_dump starts and the source is quiesced, there is no safe abort. Any failure leaves production in an undefined state.

**Context received**: No dry-run, no rollback decision tree, but logical replication is feasible. Risk confirmed.

**Specialist context questions sent to DBA / Platform Architect:**

1. Is logical replication (pglogical or built-in) testable on the target provider before committing to a cutover date?
2. What is the maximum acceptable `max_replication_lag` before cutover is considered safe?
3. Should the initial sync run during off-hours with the actual cutover window reduced to a DNS/connection-string switch (~30 seconds)?

*(Specialist answered: "Logical replication confirmed viable on target. Acceptable lag threshold: < 500ms. Initial sync: 8‚Äì12 hours during off-peak. Actual cutover: < 60 seconds connection-string switch.")*

**Re-analysis**: The rollback risk has a concrete mitigation (logical replication with instant rollback). The 4-hour window plan is still rejected because logical replication is strictly safer. Second independent Critical: the current plan has zero buffer ‚Äî no timed dry-run has been performed.

---

# üî¥ Devil's Advocate Analysis: PostgreSQL Migration Plan

**Analyzed**: 2026-02-20
**Skill version**: 2.7.10
**Scope**: Production database migration ‚Äî 2TB, new provider, 4-hour maintenance window

---

## üìä Executive Summary

**Overall Risk Rating**: üî¥ Critical

**Key Findings**:
1. Rollback is physically impossible ‚Äî restoring 2TB takes 2‚Äì4h, which cannot fit in the remaining window after any failure
2. A pg_dump of 2TB takes 2‚Äì4 hours alone ‚Äî the 4-hour window has zero buffer for validation or issues
3. A zero-downtime alternative (logical replication) exists and is not being used

**Recommendation**: ‚ùå Reject and redesign ‚Äî switch to logical replication with an off-peak initial sync and a < 60-second cutover window

**Analysis Confidence**: üü° Medium ‚Äî rollback risk confirmed with specialist; dry-run timing based on industry benchmarks (no actual measurement performed)

---

## üõë Handbrake & ‚ö° Immediate Report Status

| Protocol | Finding | Domain | Escalated to | Context received | Risk change |
|----------|---------|--------|-------------|-----------------|-------------|
| ‚ö° Immediate | No viable rollback within 4-hour window | Architecture | Team | ‚úÖ Full ‚Äî no dry-run, no decision tree | ‚û°Ô∏è Confirmed Critical |
| üõë Handbrake | No viable rollback within 4-hour window | Architecture | DBA / Platform Architect | ‚úÖ Full ‚Äî logical replication confirmed feasible | üîΩ Critical ‚Üí High with mitigation path |

**Re-analysis note**: The rollback risk has a defined mitigation (logical replication, instant rollback). The pg_dump window plan is still rejected ‚Äî logical replication is strictly safer and eliminates the irreversibility. The 4-hour window itself remains High risk due to zero buffer.

---

## ‚úÖ Strengths (What Works Well)

1. **Clear target** ‚Äî PostgreSQL 16 brings measurable performance and security improvements
2. **Sunday timing instinct is correct** ‚Äî lower traffic windows reduce user impact, even if the assumption needs validation
3. **Single ownership** ‚Äî the plan implies one team owns end-to-end execution, reducing coordination risk

---

## ‚ùå Weaknesses (What Could Fail)

### üî¥ Critical Issues (Must fix before production)

1. **4-hour window has no buffer**
   - **Risk**: pg_dump of 2TB takes 2‚Äì4 hours on commodity hardware. Add schema migration, validation, DNS cutover, and any unexpected issue ‚Äî 4 hours is best-case
   - **Impact**: Window overruns into Monday business hours with a partially migrated database and no clear recovery path
   - **Likelihood**: High ‚Äî most 2TB migrations take 6+ hours end-to-end on first attempt
   - **Mitigation**: Do a full timed dry-run on a production-sized clone before committing to the window

2. **Rollback is not feasible within the window**
   - **Risk**: If validation fails at hour 3.5, restoring a 2TB database takes another 2‚Äì4 hours ‚Äî well beyond the window
   - **Impact**: Forced to go live with unvalidated data or extend the outage indefinitely
   - **Likelihood**: Medium ‚Äî validation failures are common on first migrations
   - **Mitigation**: Switch to logical replication ‚Äî rollback becomes a connection string change (seconds, not hours)

### üü† High-Priority Issues (Should fix soon)

1. **DNS TTL split-brain at cutover**
   - **Risk**: DNS propagation delay means some app servers point to old DB, some to new. In-flight writes during this window can corrupt records in both databases
   - **Impact**: Data loss for transactions in flight during cutover
   - **Mitigation**: Pre-warm DNS TTL to 60s 48 hours before cutover; use application-level connection string switch instead of DNS

2. **Extension compatibility not validated**
   - **Risk**: Cloud providers customize PostgreSQL ‚Äî uuid-ossp, pg_trgm, PostGIS versions may differ or be absent on the target
   - **Impact**: Application failures immediately post-cutover that are hard to diagnose under pressure
   - **Mitigation**: Run a full extension compatibility matrix against the target provider before scheduling the window

### üü° Medium-Priority Issues (Technical debt)

1. **Monitoring blind spot on new provider**
   - **Risk**: Existing APM and alerting is tuned to the old provider's metrics. The team will be partially blind during the highest-risk period
   - **Mitigation**: Configure monitoring on the new provider and run it in parallel for 48 hours before cutover

---

## ‚ö†Ô∏è Assumptions Challenged

| Assumption | Challenge | Evidence | Risk if wrong |
|---|---|---|---|
| "4 hours is enough" | pg_dump alone takes 2‚Äì4h; no buffer for validation | ‚ùå No timed dry-run performed | Window overruns, Monday outage |
| "Sunday is low traffic" | B2B apps often see Sunday batch spikes; international users in different timezones | ‚ùå Not validated against traffic data | Migration runs during unexpected load spike |
| "The new provider is compatible" | Extensions and PostgreSQL configs differ across providers | ‚ùå No compatibility test run | Application failures immediately post-cutover |
| "Rollback is possible" | Restoring 2TB takes 2‚Äì4h ‚Äî cannot fit in remaining window time | ‚ùå Physically impossible at this data size | Forced to go live on corrupted data |

---

## üéØ Edge Cases & Failure Modes

| Scenario | What Happens | Handled? | Risk | Fix |
|----------|-------------|----------|------|-----|
| Migration takes 6h instead of 4h | Window overruns; old system live Monday morning with split state | ‚ùå No | Critical | Logical replication eliminates this scenario |
| Silent data corruption (encoding/locale diff) | Reports produce wrong results weeks later | ‚ùå No | Critical | Checksum validation on critical tables pre-cutover |
| ETL pipeline misconfigured for new endpoint | Nightly jobs fail silently after migration | ‚ùå No | High | Audit all downstream consumers before window |

---

## üîí Security Concerns

### STRIDE Summary
- **Spoofing**: üü¢ ‚Äî Migration uses authenticated provider credentials
- **Tampering**: üü° ‚Äî Data in transit during migration should use TLS; verify the dump/restore connection is encrypted
- **Repudiation**: üü° ‚Äî No audit log of who executed the migration or at what timestamp
- **Information Disclosure**: üü† ‚Äî 2TB dump file on intermediate storage is a data exposure risk; ensure encryption at rest and immediate deletion post-migration
- **Denial of Service**: üü† ‚Äî Migration load may saturate the old primary if no read replica is used for the dump
- **Elevation of Privilege**: üü¢ ‚Äî No new permissions being introduced

---

## ‚ö° Performance Concerns

- **Bottleneck**: pg_dump from primary saturates IOPS during the window; use a read replica for the dump if available
- **Scalability limit**: The 4-hour window is fixed by business constraints, but the data volume is not ‚Äî this plan doesn't scale beyond ~500GB safely
- **Resource usage**: DNS TTL propagation is uncontrolled ‚Äî budget 5‚Äì10 minutes of split-brain risk at cutover

---

## üí° Alternative Solutions

1. **Logical replication (pglogical or built-in)**
   - Better at: zero-downtime, instant rollback, no hard window required, can validate in parallel
   - Worse at: requires more upfront setup (replication slot, schema pre-migration)
   - Consider if: **always** ‚Äî for databases over 100GB this is the only safe approach

2. **Blue/green database switch with connection pooler (PgBouncer)**
   - Better at: instant cutover, rollback in seconds, no DNS dependency
   - Worse at: requires a connection pooler in front of the database
   - Consider if: logical replication is not available on the target provider

---

## ‚úÖ Recommendations

### Must Do (Before Production)
- [ ] Run a full timed dry-run on a production-sized clone and record actual elapsed time
- [ ] Switch to logical replication strategy ‚Äî the current plan's rollback is physically infeasible

### Should Do (Next sprint)
- [ ] Validate all extension versions on the target provider against the production schema
- [ ] Audit all downstream consumers (ETL, reporting, app servers) and verify they are reconfigured

### Consider (Backlog)
- [ ] Pre-configure monitoring and alerting on the new provider and run in parallel before cutover
- [ ] Set DNS TTL to 60s at least 48 hours before the migration window

---

## üìã Follow-Up Questions

1. Has a timed dry-run been performed on a production-sized clone to the target provider?
2. Is a read replica available to offload the dump from the primary?
3. Have all downstream systems (ETL, reporting, cron jobs, app servers) been reconfigured and tested against the new endpoint?

---
üî¥ Devil's Advocate complete.

**Before I proceed, please confirm:**
- [ ] I have reviewed all Critical and High issues above
- [ ] I accept the risks marked as accepted (or they are mitigated)
- [ ] I want to proceed with the approved action

Reply with:
  ‚úÖ Proceed ‚Äî continue with the approved action as planned
  üîÅ Revise  ‚Äî describe the change and I will re-analyse
  ‚ùå Cancel  ‚Äî stop, do not implement
  `continue` ‚Äî proceed without addressing remaining issues (risks remain active and unmitigated)
